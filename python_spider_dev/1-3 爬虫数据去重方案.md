### 1. 去重的应用场景和基本原理

#### 1.1 场景引入思考:   

- 1.防止发送重复的请求
- 2.防止保存重复的数据

> 原理:一致, 对二进制字符串去重

#### 1.2 基本原理:

思考:

- 明确 数据构成类型(字符串, 数字 , 对象, 特殊字符...)  ==> 来制定去重方案
- 判断依据: 什么样的数据算作重复数据?

> 总结:
>
> ​	根据给定的 **[判断依据]** 和 **[去重容器]**, 将原始数据逐一进行判断, 判断去重容器中是否有该数据, 如果没有则把该数据的 **对应判断依据** 添加到去重容器中, 同时标记该数据是不重复数据; 如果有就不添加, 同时标记该数据是重复数据. 
>
> - **判断依据(原始数据, 原始数据特征值)**
> - **去重容器(存储判断依据)**

#### 1.3 特征值判断原因:

- 业务需求
- 效率考量(内存中, 不能太大, 同时判断的速度要足够快)

#### 1.4 去重容器 方案选择

![去重方案的选择](./img/临时去重和持久去重的选择.png)



### 2. 基于信息摘要算法的去重

#### 2.1 信息摘要hash算法介绍

- 信息摘要hash算法可以将 **任意长度**的文本、字节数据, 通过 算法 得到一个**固定长度**的文本。如, MD5(32位16进制, 128Bit, 16Byte) 、SHA1(40位16进制, 160Bit, 20Byte)等.

  - 特征:  ==只要源文本不同, 计算得到的结果(摘要)必然不同==
  - 摘要: 摘要算法主要用于比对信息源是否一致, 因为只要源发生变化, 得到的摘要必然不同;  而且通常结果要比源短很多, 所以称为 "摘要"。

- 正因此, 利用信息摘要算法能大大降低去重容器的存储空间使用率, 并提高判断速度, 且由于其强唯一性的特征, 几乎不存在误判.

- 注意: 

  - hash算法得出的结果本质就是一串数值, 如md5的128位指的是二进制的长度, 十六进制的长度是32位. 一个16进制等于4个二进制(位数不足前面填0). 

- ```python
  In [1]: from hashlib import md5
  
  In [2]: m5 = md5()  # 小写
  
  In [3]: m5.update("abscedwwasgfsdfsd")  # python2 中这里不会报错, py3中会报错
  ---------------------------------------------------------------------------
  TypeError                                 Traceback (most recent call last)
  <ipython-input-3-6002a57d4a95> in <module>()
  ----> 1 m5.update("abscedwwasgfsdfsd")
  
  TypeError: Unicode-objects must be encoded before hashing
  
  # 对于 字符串来说 [4] 和 [5] 的操作是等价的. 
  # 中文字符串前面不能用 b"", 因为不支持 ascii码
  In [4]: m5.update("abscedwwasgfsdfsd".encode("ascii"))
  
  In [5]: m5.update(b"abscedwwasgfsdfsd")
  
  In [6]: m5.hexdigest()  # 转化成16进制
  Out[6]: '12cc79930ee8b571799afd0b0282d133'
  # Out[6] 字符串中每一个char对应的都是单独的16进制, 每个char又可以 先转换成10进制, 再10->2进制
  In [7]: int("f", 16)
  Out[7]: 15
  
  In [8]: bin(15)
  Out[8]: '0b1111'  # 0b表示是2进制
  
  In [9]: bin(5)
  Out[9]: '0b101'  # 位数不足4位, 实际操作中会在前面补0
  
  ```

#### 2.2 信息摘要hash算法去重方案实现

- 普通内存版本
  - set()    
  - 生命周期就是当前运行的程序.
- Redis持久化版本
  - 中断或者重启, 数据不丢失, 且对速度要求比较高
- MySQL持久化版本
  - 对效率要求不是特别高
  - 一般 单表加索引 可以支持 "几十万到几百万" 的业务量级.

**CODE**

- `__init__.py`

```python
# -*- coding: utf-8 -*-
# 基于mysql的去重判断依据的存储
from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

from . import BaseFilter


# class Filter(Base):
#     """"""
#     __tablename__ = "filter"
#
#     id = Column(Integer, primary_key=True)
#     hash_value = Column(String(40), index=True, unique=True)


class MySQLFilter(BaseFilter):
    """基于mysql的去重判断依据的存储"""

    def __init__(self, *args, **kwargs):
        # class Filter(Base):
        #     """"""
        #     __tablename__ = kwargs["mysql_table_name"]
        #
        #     id = Column(Integer, primary_key=True)
        #     hash_value = Column(String(40), index=True, unique=True)

        # 方法中创建类, 不太符合python风格, 可以选择使用 type 动态创建

        # self.table = Filter

        self.table = type(
            kwargs["mysql_table_name"],
            (Base,),
            dict(
                __tablename__=kwargs["mysql_table_name"],
                id=Column(Integer, primary_key=True),
                hash_value=Column(String(40), index=True, unique=True)
            )
        )

        BaseFilter.__init__(self, *args, **kwargs)

    def _get_storage(self):
        """返回一个mysql连接对象(sqlalchemy的数据库连接对象)"""
        engine = create_engine(self.mysql_url)
        Base.metadata.create_all(engine)
        Session = sessionmaker(engine)  # Session 相当于一个类
        return Session

    def _save(self, hash_value):
        """
        利用mysql(sqlachemy)
        :param hash_value:
        :return:
        """
        session = self.storage()
        filter = self.table(hash_value=hash_value)
        session.add(filter)
        session.commit()
        session.close()

    def _is_exists(self, hash_value):
        """"""
        session = self.storage()
        ret = session.query(self.table).filter_by(hash_value=hash_value).first()
        session.close()
        if ret is None:
            return False
        return True
```



### 3. 基于 Simhash 算法的去重

#### 3.1 Simhash 介绍以及应用场景

- Simhash 算法是一种局部敏感hash算法, 能实现 **相似** 文本内容的去重。

#### 3.2 Simhash的特征

- 与信息摘要算法的区别

  | 信息摘要算法                                            | Simhash算法                                         |
  | ------------------------------------------------------- | --------------------------------------------------- |
  | 如果原始内容只相差一个字节, 所产生的签名也可能差别很大. | 如果原始内容只相差一个字节, 所产生的签名差别非常小. |

- **海明距离**: 比较两者的simhash值的**二进制位**的差异来表示原始文本内容的差异. 差异个数又被称为**海明距离.**

- 注意:

  - Simhash 对长文本 500字+ 比较适用, 短文本可能偏差较大
  - 在Google的论文给出的数据中, 64位simhash值, 在海明距离为3(<=3)的情况下, 可以认为两篇文档是相似的或者是重复的. 当然这个值只是参考值, 针对自己的应用可能有不同的测试取值.

#### 3.3 Simhash值得比对

- Python实现的 [simhash](https://github.com/leonsim/simhash) 算法.  该模块得出的simhash值长度正是64位
- 64 或者 其他位数 都可以, 但是进行对比的 长度必须一致



#### 